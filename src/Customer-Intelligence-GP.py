import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from scipy.stats import norm
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel
import joblib
from mpl_toolkits.mplot3d import Axes3D

# ============================================
# CONFIGURATION
# ============================================
AUTO_MODE = True  # Run automatically (non-interactive)

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '..'))
DATA_DIR = os.path.join(BASE_DIR, 'data')
OUTPUT_DIR = os.path.join(BASE_DIR, 'outputs')

DATA_FILE = 'Online Retail.xlsx'
file_path = os.path.join(DATA_DIR, DATA_FILE)

# Output paths
csv_output_path = os.path.join(OUTPUT_DIR, 'customers_segmented_v2.csv')
xlsx_output_path = os.path.join(OUTPUT_DIR, 'customers_segmented_v2.xlsx')
metrics_output_path = os.path.join(OUTPUT_DIR, 'kmeans_metrics_v2.csv')
cluster_profile_path = os.path.join(OUTPUT_DIR, 'cluster_profiles_v2.csv')
distribution_dir = os.path.join(OUTPUT_DIR, 'distributions')
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(distribution_dir, exist_ok=True)

# ============================================
# LOAD & CLEAN DATA
# ============================================
if not os.path.exists(file_path):
    raise FileNotFoundError(f"‚ùå Data file not found: {file_path}")

print("üì¶ Loading and preparing data...")
data = pd.read_excel(file_path)
data.drop_duplicates(inplace=True)
data.dropna(subset=['CustomerID'], inplace=True)
data['total_spent'] = data['UnitPrice'] * data['Quantity']
data = data[data['total_spent'] > 0]
data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])
reference_date = data['InvoiceDate'].max()

# ============================================
# CREATE RFM FEATURES
# ============================================
print("üßÆ Generating RFM metrics...")
rfm = data.groupby('CustomerID').agg(
    Recency=('InvoiceDate', lambda x: (reference_date - x.max()).days),
    Frequency=('InvoiceNo', 'nunique'),
    Monetary=('total_spent', 'sum')
).reset_index()

# Standardize
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])

# ============================================
# FIND OPTIMAL NUMBER OF CLUSTERS
# ============================================
print("üîç Finding optimal cluster count (KMeans)...")
inertia, silhouette_scores = [], []
k_range = range(2, 11)
for k in k_range:
    km = KMeans(n_clusters=k, n_init=10, random_state=42)
    labels = km.fit_predict(rfm_scaled)
    inertia.append(km.inertia_)
    silhouette_scores.append(silhouette_score(rfm_scaled, labels))

metrics_df = pd.DataFrame({'k': list(k_range), 'Inertia': inertia, 'Silhouette': silhouette_scores})
metrics_df.to_csv(metrics_output_path, index=False)

optimal_k = np.argmax(silhouette_scores) + 2
print(f"‚úÖ Optimal number of clusters determined: {optimal_k}")

# ============================================
# FINAL CLUSTERING
# ============================================
kmeans = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)
rfm['Segment'] = kmeans.fit_predict(rfm_scaled)
centroids = scaler.inverse_transform(kmeans.cluster_centers_)
joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'rfm_scaler.pkl'))
joblib.dump(kmeans, os.path.join(OUTPUT_DIR, 'kmeans_model.pkl'))

# ============================================
# PCA FOR VISUALIZATION
# ============================================
pca = PCA(n_components=2, random_state=42)
rfm_pca = pca.fit_transform(rfm_scaled)
rfm['PCA1'], rfm['PCA2'] = rfm_pca[:, 0], rfm_pca[:, 1]

# ============================================
# CLUSTER SUMMARY
# ============================================
cluster_summary = rfm.groupby('Segment').agg({
    'Recency': 'mean', 'Frequency': 'mean', 'Monetary': 'mean', 'CustomerID': 'count'
}).rename(columns={'CustomerID': 'Count'}).round(1)

def assign_segment_labels(summary_df):
    labels = {}
    monetary_rank = summary_df['Monetary'].rank(method='min', ascending=False)
    recency_rank = summary_df['Recency'].rank(method='min')
    frequency_rank = summary_df['Frequency'].rank(method='min', ascending=False)
    for idx in summary_df.index:
        if monetary_rank[idx] == 1:
            labels[idx] = 'High Value'
        elif frequency_rank[idx] == 1:
            labels[idx] = 'Loyal'
        elif recency_rank[idx] == summary_df['Recency'].rank().max():
            labels[idx] = 'Churn Risk'
        else:
            labels[idx] = 'Regular'
    return labels

segment_labels = assign_segment_labels(cluster_summary)
rfm['SegmentLabel'] = rfm['Segment'].map(segment_labels)
cluster_summary['Label'] = cluster_summary.index.map(segment_labels)
cluster_summary.to_csv(cluster_profile_path)

# ============================================
# DISTRIBUTION PLOTS
# ============================================
def plot_distribution_with_iqr(data, column):
    plt.figure(figsize=(7, 4))
    sns.histplot(data[column], bins=30, kde=True, color='skyblue', stat='density', edgecolor='black')
    mu, std = norm.fit(data[column])
    x = np.linspace(data[column].min(), data[column].max(), 100)
    p = norm.pdf(x, mu, std)
    plt.plot(x, p, 'r--', label='Gaussian Fit')
    q1, q3 = np.percentile(data[column], [25, 75])
    plt.axvline(q1, color='green', linestyle='--', label='Q1 (25%)')
    plt.axvline(q3, color='purple', linestyle='--', label='Q3 (75%)')
    plt.title(f'{column} Distribution with Gaussian Fit')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(distribution_dir, f"{column}_distribution.png"), dpi=200)
    plt.close()

for col in ['Recency', 'Frequency', 'Monetary']:
    plot_distribution_with_iqr(rfm, col)

# ============================================
# TABLEAU-READY CLUSTER VISUALIZATION
# ============================================
print("üé® Generating Tableau-ready segmentation charts...")

segment_palette = {
    'High Value': '#FF6B6B',   # Red
    'Loyal': '#4ECDC4',        # Teal
    'Regular': '#FFD93D',      # Yellow
    'Churn Risk': '#1A535C'    # Dark Blue
}

# --- RFM Chart ---
plt.figure(figsize=(8,5))
for label, color in segment_palette.items():
    subset = rfm[rfm['SegmentLabel'] == label]
    plt.scatter(subset['Recency'], subset['Monetary'],
                label=label, color=color, alpha=0.7, edgecolor='k', s=60)

centroids_df = pd.DataFrame(centroids, columns=['Recency', 'Frequency', 'Monetary'])
plt.scatter(centroids_df['Recency'], centroids_df['Monetary'],
            color='red', marker='X', s=200, label='Centroids')

plt.title('Customer Segments (RFM)')
plt.xlabel('Recency (days since last purchase)')
plt.ylabel('Monetary (total spend)')
plt.legend(title='Segment', fontsize=9)
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'rfm_segmentation_tableau_ready.png'), dpi=200)
plt.close()

# --- PCA Chart ---
plt.figure(figsize=(8,5))
for label, color in segment_palette.items():
    subset = rfm[rfm['SegmentLabel'] == label]
    plt.scatter(subset['PCA1'], subset['PCA2'], 
                label=label, color=color, alpha=0.8, edgecolor='k', s=60)

plt.title('Customer Segments (PCA Visualization)')
plt.xlabel('PCA1 (Customer Value Dimension)')
plt.ylabel('PCA2 (Engagement Dimension)')
plt.legend(title='Segment', fontsize=9)
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'pca_segmentation_tableau_ready.png'), dpi=200)
plt.close()

# ============================================
# GAUSSIAN PROCESS REGRESSION (GPR)
# ============================================
print("üîÑ Training Gaussian Process Regression (GPR)...")
X = rfm_scaled[:, :2]  # Recency & Frequency
y = rfm['Monetary'].values

kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel()
gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=3, random_state=42)
gpr.fit(X, y)
print(f"‚úÖ GPR trained. Kernel: {gpr.kernel_}")

y_pred, y_std = gpr.predict(X, return_std=True)
rfm['GPR_Predicted_Monetary'] = y_pred
rfm['GPR_Uncertainty'] = y_std

# --- GPR Surface Plot ---
r = np.linspace(X[:, 0].min(), X[:, 0].max(), 50)
f = np.linspace(X[:, 1].min(), X[:, 1].max(), 50)
R, F = np.meshgrid(r, f)
X_grid = np.column_stack([R.ravel(), F.ravel()])
Y_mean, Y_std = gpr.predict(X_grid, return_std=True)
Y_mean, Y_std = Y_mean.reshape(R.shape), Y_std.reshape(R.shape)

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(R, F, Y_mean, cmap='viridis', alpha=0.8)
ax.set_xlabel('Recency (scaled)')
ax.set_ylabel('Frequency (scaled)')
ax.set_zlabel('Predicted Monetary')
ax.set_title('GPR Predicted Monetary Surface')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'gpr_predicted_surface.png'), dpi=200)
plt.close()

# --- GPR Uncertainty Heatmap ---
plt.figure(figsize=(6,5))
plt.contourf(R, F, Y_std, levels=20, cmap='coolwarm')
plt.colorbar(label='Uncertainty (std)')
plt.xlabel('Recency (scaled)')
plt.ylabel('Frequency (scaled)')
plt.title('GPR Prediction Uncertainty')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'gpr_uncertainty_heatmap.png'), dpi=200)
plt.close()

# ============================================
# EXPORT DATA
# ============================================
rfm.to_csv(csv_output_path, index=False)
rfm.to_excel(xlsx_output_path, index=False)

print(f"""
‚úÖ RFM segmentation and GPR complete!
------------------------------------
üìä Tableau-ready outputs:
  ‚Ä¢ CSV:  {csv_output_path}
  ‚Ä¢ XLSX: {xlsx_output_path}

üìà Charts generated:
  ‚Ä¢ RFM Segmentation:      rfm_segmentation_tableau_ready.png
  ‚Ä¢ PCA Segmentation:      pca_segmentation_tableau_ready.png
  ‚Ä¢ GPR Surface:           gpr_predicted_surface.png
  ‚Ä¢ GPR Uncertainty:       gpr_uncertainty_heatmap.png
  ‚Ä¢ Distributions:         {distribution_dir}

üìÑ Cluster Summary:
{cluster_summary[['Label', 'Recency', 'Frequency', 'Monetary', 'Count']]}
""")
