# ===============================================================
# FULL DATA SCIENCE PIPELINE FOR CUSTOMER SEGMENTATION & INSIGHTS
# ===============================================================
# Includes:
#  ‚Ä¢ Data Loading & Cleaning
#  ‚Ä¢ RFM Feature Engineering
#  ‚Ä¢ KMeans Clustering with Optimal Cluster Selection
#  ‚Ä¢ PCA (2D & 3D Visualization)
#  ‚Ä¢ Gaussian Process Regression (GPR) for Spending Prediction
#  ‚Ä¢ Segmentation Labeling (High Value, Loyal, Regular, Churn Risk)
#  ‚Ä¢ Plot Generation (RFM, PCA 2D/3D, GPR, Distributions)
#  ‚Ä¢ Export to CSV and XLSX
# ---------------------------------------------------------------
# Author: [Your Name]
# Version: Master Pipeline v3.0
# Date: [Autogenerated]
# ---------------------------------------------------------------

import os  # File and directory management
import pandas as pd  # Data manipulation and analysis
import numpy as np  # Numerical computations
import matplotlib.pyplot as plt  # Plotting and visualization
import seaborn as sns  # Advanced plotting
from sklearn.preprocessing import StandardScaler  # Feature scaling for clustering
from sklearn.cluster import KMeans  # KMeans clustering algorithm
from sklearn.metrics import silhouette_score, mean_squared_error, r2_score  # Evaluation metrics
from sklearn.decomposition import PCA  # Dimensionality reduction
from sklearn.gaussian_process import GaussianProcessRegressor  # Gaussian Process Regression model
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel  # Kernels for GPR
from scipy.stats import norm  # Normal distribution fitting
import joblib  # Model saving/loading
from mpl_toolkits.mplot3d import Axes3D  # For 3D visualization

# ============================================
# CONFIGURATION
# ============================================
AUTO_MODE = True  # Automatically run without user input

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))  # Get current script directory
BASE_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '..'))  # Go one directory up for base
DATA_DIR = os.path.join(BASE_DIR, 'data')  # Data input folder
OUTPUT_DIR = os.path.join(BASE_DIR, 'outputs')  # Output folder for all results
DISTRIBUTION_DIR = os.path.join(OUTPUT_DIR, 'distributions')  # Folder for distribution plots

os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create output folder if missing
os.makedirs(DISTRIBUTION_DIR, exist_ok=True)  # Create distributions folder

DATA_FILE = 'Online Retail.xlsx'  # Expected dataset file
DATA_PATH = os.path.join(DATA_DIR, DATA_FILE)  # Full dataset path

# Output file paths
csv_output_path = os.path.join(OUTPUT_DIR, 'customers_segmented_master.csv')
xlsx_output_path = os.path.join(OUTPUT_DIR, 'customers_segmented_master.xlsx')
metrics_output_path = os.path.join(OUTPUT_DIR, 'kmeans_metrics_master.csv')
cluster_profile_path = os.path.join(OUTPUT_DIR, 'cluster_profiles_master.csv')

# ============================================
# LOAD & CLEAN DATA
# ============================================
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"‚ùå Data file not found: {DATA_PATH}")  # Stop if missing

print("üì¶ Loading and cleaning data...")
data = pd.read_excel(DATA_PATH)  # Load dataset from Excel file
data.drop_duplicates(inplace=True)  # Remove duplicate rows
data.dropna(subset=['CustomerID'], inplace=True)  # Drop missing Customer IDs
data['total_spent'] = data['UnitPrice'] * data['Quantity']  # Compute total spending
data = data[data['total_spent'] > 0]  # Keep only positive transactions
data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])  # Convert to datetime
reference_date = data['InvoiceDate'].max()  # Most recent date as reference

# ============================================
# RFM FEATURE ENGINEERING
# ============================================
print("üßÆ Calculating RFM metrics (Recency, Frequency, Monetary)...")
rfm = data.groupby('CustomerID').agg(
    Recency=('InvoiceDate', lambda x: (reference_date - x.max()).days),  # Days since last purchase
    Frequency=('InvoiceNo', 'nunique'),  # Count of unique invoices
    Monetary=('total_spent', 'sum')  # Total amount spent
).reset_index()

# Standardize RFM features for clustering
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])

# ============================================
# DETERMINE OPTIMAL CLUSTER COUNT
# ============================================
print("üîç Finding optimal K for KMeans clustering...")
inertia = []  # Sum of squared distances (compactness)
silhouette_scores = []  # Separation and cohesion measure

for k in range(2, 11):
    km = KMeans(n_clusters=k, n_init=10, random_state=42)  # Initialize model
    labels = km.fit_predict(rfm_scaled)  # Fit and predict cluster labels
    inertia.append(km.inertia_)  # Append inertia
    silhouette_scores.append(silhouette_score(rfm_scaled, labels))  # Append silhouette score

metrics_df = pd.DataFrame({'k': range(2, 11), 'Inertia': inertia, 'Silhouette': silhouette_scores})
metrics_df.to_csv(metrics_output_path, index=False)  # Save metrics for review

optimal_k = np.argmax(silhouette_scores) + 2  # Get best k value (+2 offset since range starts at 2)
print(f"‚úÖ Optimal number of clusters determined: {optimal_k}")

# ============================================
# FINAL CLUSTERING
# ============================================
print("ü§ñ Running final KMeans clustering...")
kmeans = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)
rfm['Segment'] = kmeans.fit_predict(rfm_scaled)  # Assign cluster labels
centroids = scaler.inverse_transform(kmeans.cluster_centers_)  # Convert centroids back to original scale

joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'rfm_scaler.pkl'))  # Save scaler
joblib.dump(kmeans, os.path.join(OUTPUT_DIR, 'kmeans_model.pkl'))  # Save model

# ============================================
# PCA DIMENSIONALITY REDUCTION (2D + 3D)
# ============================================
print("üé® Applying PCA for dimensionality reduction and visualization...")
pca = PCA(n_components=3, random_state=42)  # Use 3 components for 3D visualization
rfm_pca = pca.fit_transform(rfm_scaled)  # Fit and transform data
rfm['PCA1'], rfm['PCA2'], rfm['PCA3'] = rfm_pca[:, 0], rfm_pca[:, 1], rfm_pca[:, 2]  # Add to dataframe

# ============================================
# CLUSTER SUMMARY
# ============================================
cluster_summary = rfm.groupby('Segment').agg({
    'Recency': 'mean', 'Frequency': 'mean', 'Monetary': 'mean', 'CustomerID': 'count'
}).rename(columns={'CustomerID': 'Count'}).round(1)

def assign_segment_labels(summary_df):
    """Assigns human-readable labels based on RFM metrics"""
    labels = {}
    monetary_rank = summary_df['Monetary'].rank(method='min', ascending=False)
    recency_rank = summary_df['Recency'].rank(method='min')
    frequency_rank = summary_df['Frequency'].rank(method='min', ascending=False)
    for idx in summary_df.index:
        if monetary_rank[idx] == 1:
            labels[idx] = 'High Value'
        elif frequency_rank[idx] == 1:
            labels[idx] = 'Loyal'
        elif recency_rank[idx] == summary_df['Recency'].rank().max():
            labels[idx] = 'Churn Risk'
        else:
            labels[idx] = 'Regular'
    return labels

segment_labels = assign_segment_labels(cluster_summary)
rfm['SegmentLabel'] = rfm['Segment'].map(segment_labels)
cluster_summary['Label'] = cluster_summary.index.map(segment_labels)
cluster_summary.to_csv(cluster_profile_path)

# ============================================
# DISTRIBUTION PLOTS (Recency, Frequency, Monetary)
# ============================================
def plot_distribution_with_iqr(data, column):
    """Plot histogram with Gaussian fit and IQR lines"""
    plt.figure(figsize=(7, 4))
    sns.histplot(data[column], bins=30, kde=True, color='skyblue', stat='density', edgecolor='black')
    mu, std = norm.fit(data[column])
    x = np.linspace(data[column].min(), data[column].max(), 100)
    plt.plot(x, norm.pdf(x, mu, std), 'r--', label='Gaussian Fit')
    q1, q3 = np.percentile(data[column], [25, 75])
    plt.axvline(q1, color='green', linestyle='--', label='Q1 (25%)')
    plt.axvline(q3, color='purple', linestyle='--', label='Q3 (75%)')
    plt.title(f'{column} Distribution with Gaussian Fit')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(DISTRIBUTION_DIR, f"{column}_distribution.png"), dpi=200)
    plt.close()

for col in ['Recency', 'Frequency', 'Monetary']:
    plot_distribution_with_iqr(rfm, col)

# ============================================
# TABLEAU-READY COLOR PALETTE
# ============================================
segment_palette = {
    'High Value': '#FF6B6B',
    'Loyal': '#4ECDC4',
    'Regular': '#FFD93D',
    'Churn Risk': '#1A535C'
}

# ============================================
# VISUALIZATION: 2D PCA & RFM PLOTS
# ============================================
print("üìä Generating PCA and RFM visualizations...")

# 2D PCA Visualization
plt.figure(figsize=(8, 5))
for label, color in segment_palette.items():
    subset = rfm[rfm['SegmentLabel'] == label]
    plt.scatter(subset['PCA1'], subset['PCA2'], label=label, color=color, alpha=0.8, edgecolor='k', s=60)
plt.title('Customer Segments (PCA 2D)')
plt.xlabel('PCA1 (Customer Value)')
plt.ylabel('PCA2 (Engagement)')
plt.legend(title='Segment', fontsize=9)
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'pca_segmentation_2d.png'), dpi=200)
plt.close()

# 3D PCA Visualization
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')
for label, color in segment_palette.items():
    subset = rfm[rfm['SegmentLabel'] == label]
    ax.scatter(subset['PCA1'], subset['PCA2'], subset['PCA3'],
               label=label, color=color, s=60, edgecolor='k', alpha=0.8)
ax.set_title('Customer Segments (PCA 3D)')
ax.set_xlabel('PCA1')
ax.set_ylabel('PCA2')
ax.set_zlabel('PCA3')
ax.legend(title='Segment', fontsize=9)
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'pca_segmentation_3d.png'), dpi=200)
plt.close()

# ============================================
# GAUSSIAN PROCESS REGRESSION (GPR)
# ============================================
print("üî• Training Gaussian Process Regression for Monetary Prediction...")

X = rfm_scaled[:, :2]  # Recency & Frequency (scaled)
y = rfm['Monetary'].values  # Target: actual Monetary

kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel()
gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=3, random_state=42)
gpr.fit(X, y)  # Train model

y_pred, y_std = gpr.predict(X, return_std=True)  # Predict Monetary and uncertainty
rfm['GPR_Predicted_Monetary'] = y_pred
rfm['GPR_Uncertainty'] = y_std

rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)
print(f"‚úÖ GPR trained successfully ‚Äî RMSE: {rmse:.2f}, R¬≤: {r2:.4f}")
print(f"Kernel used: {gpr.kernel_}")

# GPR 3D Surface Plot
r = np.linspace(X[:, 0].min(), X[:, 0].max(), 50)
f = np.linspace(X[:, 1].min(), X[:, 1].max(), 50)
R, F = np.meshgrid(r, f)
X_grid = np.column_stack([R.ravel(), F.ravel()])
Y_mean, Y_std = gpr.predict(X_grid, return_std=True)
Y_mean, Y_std = Y_mean.reshape(R.shape), Y_std.reshape(R.shape)

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(R, F, Y_mean, cmap='viridis', alpha=0.8)
ax.set_xlabel('Recency (scaled)')
ax.set_ylabel('Frequency (scaled)')
ax.set_zlabel('Predicted Monetary')
ax.set_title('GPR Predicted Monetary Surface')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'gpr_predicted_surface.png'), dpi=200)
plt.close()

# GPR Uncertainty Heatmap
plt.figure(figsize=(6, 5))
plt.contourf(R, F, Y_std, levels=20, cmap='coolwarm')
plt.colorbar(label='Prediction Std. Dev. (Uncertainty)')
plt.xlabel('Recency (scaled)')
plt.ylabel('Frequency (scaled)')
plt.title('GPR Prediction Uncertainty')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'gpr_uncertainty_heatmap.png'), dpi=200)
plt.close()

# ============================================
# EXPORT DATA
# ============================================
rfm.to_csv(csv_output_path, index=False)
rfm.to_excel(xlsx_output_path, index=False)

# ============================================
# FINAL OUTPUT SUMMARY
# ============================================
print(f"""
‚úÖ FULL DATA PIPELINE COMPLETE
------------------------------------
üìä Outputs:
  ‚Ä¢ Clustered Customers CSV:  {csv_output_path}
  ‚Ä¢ Clustered Customers XLSX: {xlsx_output_path}
  ‚Ä¢ Cluster Summary:          {cluster_profile_path}

üìà Visualizations:
  ‚Ä¢ RFM Segmentation
  ‚Ä¢ PCA 2D & 3D Segmentation
  ‚Ä¢ GPR Surface & Uncertainty Maps
  ‚Ä¢ Distribution Plots:       {DISTRIBUTION_DIR}

üìÑ Cluster Summary:
{cluster_summary[['Label', 'Recency', 'Frequency', 'Monetary', 'Count']]}
""")
